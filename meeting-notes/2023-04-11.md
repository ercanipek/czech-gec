mixed policy mixed_float16 - na trenink uplne staci (mixed precision)
zkusit Adafactor - hlavne na nepredtrenovany (porovnat Adafactor a Adam na predtrenovanem)

stahnout gecc pro evaluaci - chat Kuba
maxmatch score pro evaluaci
bude vice datasetu pro vyhodnoceni
zkusit lr = 0.001, 0.0001, ...
zkusit model nepretrenovany 
mt5 zkusit dotrenovat na train in gecc

news - moje data nejsou tokenizovana, ale skript od Kuby vyzaduje tokenizovane, kouknout se, jestli to dela problemy.
-> skript na tokenizovani - /home/straka/students/pechman/udpipe_tokenizer

pouzivaji 0.001, ale maji o 2 rady vetsi batch_size

1. Evaluace
2. Fine-tuning - 1 den
3. Rozdelit vety podle poctu subwordu a rozdelit to kategorii podle rozmezi delek a dle toho plnit batch - je to v tensorflow (clanky mt5, atd.) bucket by sequence length
4. Trenovani from scratch (nepredtrenovany)