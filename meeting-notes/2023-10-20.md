notes:
- Adafactor - vetsi batche
- Mensi beta 0.98 u adaptive Adam
- zkusit porovnat


- vzit skript o Kuby a vytvorit offline data (puvodni aspell)

- zkusit i tokenizer z Kubovo dat
---
- konstanti lr (decay az pozdeji, mensi nez trenovatelna, 1/5), projit data vickrat 
- finetuning - cista data akces
- 1:2 - michani dat (cista, synteticka)
- cca 80%

---
- zkusit mt5-base

---
podivat se na chyby

1. Trenovani, krom finetuning
2. Finetuning
3. Generovani dat
4. Chyby

---
- zkusit i Bytovy tokenizer
- ByT5 ma takovy tokenizer v sobe (budou delsi vstupy)

---

Dalsi meeting:
- patek 10:30

1. Co lze spustit rovnou:
   - `bart-szn-2` - training - bart - AdamW - novy tokenizer - Kubova data
   - `bart-szn-2-adafactor` - training - mt5-base - Adafactor - novy tokenizer - Kubovo data (kouknout na parametry do clanku - nenasel jsem)
2. Mozna tvorba dat:
   - finetuning - bart - AdamW - cista akces train data - lr: 1/5 puvodniho
   - finetuning - bart - AdamW - mix dat (1(cista):2(synteticka)) - lr: 1/5 puvodniho
3. Generovani offline dat z Kubovo skriptu (puvodni aspell) -> zkusit najÃ­t chybu
4. 