notes:
- Adafactor - vetsi batche
- Mensi beta 0.98 u adaptive Adam
- zkusit porovnat
- vzit skript o Kuby a vytvorit offline data (puvodni aspell)
- zkusit i tokenizer z Kubovo dat
---
- konstanti lr (decay az pozdeji, mensi nez trenovatelna, 1/5), projit data vickrat 
- finetuning - cista data akces
- 1:2 - michani dat (cista, synteticka)
- cca 80%
---
- zkusit mt5-base
- podivat se na chyby
---
- zkusit i Bytovy tokenizer
- ByT5 ma takovy tokenizer v sobe (budou delsi vstupy)
---

1. Trenovani, krom finetuning
2. Finetuning
3. Generovani dat
4. Chyby

---

Dalsi meeting:
- patek 10:30

---

1. Co lze spustit rovnou:
   - `bart-szn-2` - training - bart - AdamW - novy tokenizer - Kubova data
   - `bart-szn-2-adafactor` - training - mt5-base - Adafactor - novy tokenizer - Kubovo data (spatny config -> nefungovalo)
2. Mozna tvorba dat:
   - `bart-szn-2-finetuning` - finetuning - bart - AdamW - cista akces train data - lr: 1/5 puvodniho
   - `bart-szn-2-finetuning-15M` - finetuning - bart - AdamW - mix dat (1(cista):2(synteticka)) - lr: 1/5 puvodniho
3. Generovani offline dat z Kubovo skriptu (puvodni aspell) -> zkusit najít chybu
   - Porovnat GEL a skript
   - introduce_errors.py je úplně jiný
   - `bart-szn-2-pipeline` - se starým introduce_errors.py

---

Zjisteni:
- ```python
  line = line[:-1] if line[-1] == "\n" else line
  ```