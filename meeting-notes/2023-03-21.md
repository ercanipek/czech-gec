- learning rate
  - předtrénovaný model => menší lr (0.00001)
  - nově trénovaný mode => začít s větším lr
- menší MAX_LENGTH (cca. 128, ale i třeba 64), pak možnost použít větší batch size
- používat dynamicky velké tokenizované věty a až potom batch, který je možno zarovnat dle nejmenšího, nebo je přeskupit tak, aby byly u sebe stejné velikosti
- postupné zvětšování lr a pak klesat - Decay (článek o transformerech, DL slidy)
- MT5Tokenizer vyžaduje balíček sentencepiece
- změnit tf.py_function na tf.np_function
- přidat shuffle (1000, 500 - buffer size)
- nedělit dělení pomocí chunks, ale použít fit - epoch_steps (počet batchů na 1 epchu, callback)
- map - použít num_parallel_cells=tf.data.AUTOTUNE
- nvidia-smi - vytížení grafik, tf umí profilovat -> profile batch (1. až 10. batch)
- dělat si průběžnou evaluaci (metriky) - jiný porces, sidecar evaluation v tf
- jen si vypsat, jak vypadá predict
- zkusit small, pak base
- vyzkoušet mt5
- pokud nebude fungovat zkusit identitu (1h fine-tuning)
- AdaFactor ušetří trochu místa na batch size
  