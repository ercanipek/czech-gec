`bart-szn-2` - training - bart - AdamW - novy tokenizer - Kubova data (24. checkpoint)
`bart-szn-2-backup` - ulozeny 10. checkpoint

`bart-szn-3-params` - data 2017 - pipeline - puvodni introduce_errors - parametry pro kažení dat z Kubovo práce (16. checkpoint)
`bart-szn-3-params-copy` - ulozeny 10. checkpoint

(`bart-szn-3-new-errors` - data 2017 - pipeline - novy introduce_errors - parametry pro kažení dat z Kubovo práce)

---

- jeste chvili spustit `bart-szn-3-params` - mit starsi checkpoint (vic jak 16) -> Mam 25. checkpoint

- vzit posledni checkpoint nejlepsich (staticka data i pipelina) - finetuning - 1/5 lr
  - spis staticka data (24. checkpoint)
  - `bart-szn-5-static-finetuning-lr-5` - staticka data pri predtreninku (z `bart-szn-2`)
  - `bart-szn-5-pipeline-finetuning-lr-5` - pipelina chyb (z `bart-szn-3-params`)

- vzit posledni checkpoint nejlepsich (staticka data i pipelina) - finetuning - 1/10 lr
  - spis staticka data (24. checkpoint)
  - `bart-szn-5-static-finetuning-lr-10` - staticka data pri predtreninku (z `bart-szn-2`)
  - `bart-szn-5-pipeline-finetuning-lr-10` - pipelina chyb (z `bart-szn-3-params`)
  
- vzit posledni checkpoint nejlepsich (staticka data i pipelina) - finetuning - 1/25 lr
  - spis staticka data (24. checkpoint)
  - `bart-szn-5-static-finetuning-lr-25` - staticka data pri predtreninku (z `bart-szn-2`)
  - `bart-szn-5-pipeline-finetuning-lr-25` - pipelina chyb (z `bart-szn-3-params`)


- predelat evaluaci, aby ukladala predikovane vety

- obracena pipelina -> `bart-szn-5-reverted-pipeline`
  - `bart-szn-5-reverted-pipeline-finetuning`
  - `bart-szn-5-generate-errors`

(21. 11. - pustit finetuning pro `bart-szn-3-params` - starsi, obracena pipelina)

---
#
- edity delame, abychom dostali m2
- zakazat edity, ktere se prekryvaji (pomoci sampler) 
- nepouzivat errant
#



- nastroj na generovani chyb
  - do syntetickeho vytvareni chyb (nepotrebuji m2 edity, potrebuji procentualni zastopeni chyby)
  - samostatne na vytvoreni datasetu (potrebuje m2 edity, potrebuji procentualni zastopeni chyby)
- nastroj na pretagovani chyb v akces, geccc
  (- pregenerovat akces pomoci erratnu)
  - pretagovat chyby v "error - golden" a "error - predicted" (na typicke (moje) chyby)
  - pouzit compare_m2 na "error - golden" a "error - predicted"


- bug: 158 new_char = current_char + ' ' + np.random.choice(char_vocabulary)
  - opravit jen na zmenu znaku a insert uz ma mezeru
  - porovnat, jestli se to zlepsilo

- chyba (do editu) - vlozeny znak/preklep (jen na alpha) - hlavne klasifikovat (abychom vedeli, kolik jich tam je)

---


---

tokenizer.decode(tokenizer.encode("Ahoj , jak se máš ?")) - nedela mezery