- nechat generate - batch pro evaluaci
- maskovani u loss
- transformer - tf keral nlp - random vahy (viz web), mozna 
- word dropout - asi bude snazsi u keras nlp transformeru
- GPU zkontrolovat vytizeni

- logovani skore do tensorboardu

- sestaveni batche - keras nlp multisegment-packer, koordinovane velikosti (2 zpusoby)

- modely: mt5, random(tf.k.nlp), byteT5

- tokenizer pro tf keras nlp transformer - byte, word

- checkpointovat model i optimizer

- dataset repeat


- lr dle clanku:
  (Kuba chat)
  We use Adafactor optimizer (Shazeer and Stern,
  2018), linearly increasing the learning rate from 0
  to 0.011 over the first 8000 steps, then decrease
  it proportionally to the number of steps after that
  (using the rsqrt_decay schedule). Note that
  this only applies to the pre-training phase

- https://arxiv.org/pdf/1910.00353.pdf

- dat si bacha na ukladani optimizeru (nove verze pro tf)

- tensor2tensor - beta2 - nebo clanek - vychozi je moc velka - 0.997

- poznamka:
- nemusi fungovat decode po generate, protoze tokenizer nemusi pridat mezery(pr. "Ahoj ," vs "Ahoj,")