- set -e zastavi bash(uz je v generate_data_wrapper)
- sbatch poustet samostatne v generate_wrapper ("odkomentovat")
- MarkDown
- zkusit 2 GPU karty - distributed training
- negenerovat velky file, ale i dataset behem trenovani
- AdaFactor, AdamW(z kerasu)
- keras NLP balicek
- jenom v keras
- jiny batch_size=64 (128) - asi to bude dost
- strategie trenovani - tf.distribute.MirrorStrategy
- tf.data.TextLineDataset (tokenizer)
- config - AutoConfig (pretahne jenom architekturu)
- mBart, BART, mt5(pretrenovana)
- vetsi lr
- zanechat vocablurary (log, lin probs pro insert)
- insert jen pro stejne slovo z vety
- replace je aspell
- ByT5 - stejne jako t5(ale vetsi)
- (accumulation gradients)