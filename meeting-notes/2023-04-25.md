- mt5 - problem pri mixed_float16
- na mixed_float16 je node2 lepsi

- generate - zkusit vetsi batch size (pouziti vice GPU bude chtit zkusit, kdyz tak rucne)
- create_error_line - trvá moc dlouho (paralelni zpracovani dat, vlastni skoceni do mista ve file, from generator, DL04) seek nahodne a zbytku procesu dopocitat pravidelne


- inheritance transformer from scratch - opakované počítání s output zabere hrozně času, rychlá implementace v HuggingFace
- proto uděláme TFBart from scratch (generate už udělaný) - pouzit init a svuj config pro vytvoreni klasickeho transformeru - DP, clanek (AIAYN)
- tokenizer, ktery ignoruje mezery - natrenovat na vlastnich datech - create wordpiece tokenizer - vocabsize ~ cca. 32_000

- batch: 
    1. bucket by sequence length 
    2. udelani masky pro batch

- sesortit evaluacni data -> rychlejsi vypocet

- vytahnout evaluaci ven (hlavne m2scorer) -> zalogovat skore zpatky do tensorboardu

- tf.keras.callbacks.BackAndRestore - drzi chceckpoit na zachranu