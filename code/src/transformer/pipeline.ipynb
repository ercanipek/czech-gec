{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATOR = False\n",
    "if len(sys.argv) > 1:\n",
    "    if sys.argv[1] == \"-e\":\n",
    "        EVALUATOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# from m2scorer.util import paragraphs\n",
    "# from m2scorer.util import smart_open\n",
    "# from m2scorer.levenshtein import batch_multi_pre_rec_f1\n",
    "# from m2scorer.m2scorer import load_annotation\n",
    "\n",
    "# from tensorflow.keras import mixed_precision\n",
    "\n",
    "from utils import load_data\n",
    "from utils import introduce_errors \n",
    "\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "SEED = config['seed']\n",
    "\n",
    "# data loading\n",
    "DATA_PATHS = config['data_paths']\n",
    "NUM_PARALLEL = config['num_parallel']\n",
    "MAX_LENGTH = config['max_length']\n",
    "SHUFFLE_BUFFER = config['shuffle_buffer']\n",
    "\n",
    "# model\n",
    "MODEL = config['model']\n",
    "PRETRAINED = config['pretrained']\n",
    "STEPS_PER_EPOCH = config['steps_per_epoch']\n",
    "EPOCHS = config['epochs']\n",
    "\n",
    "# optimizer\n",
    "OPTIMIZER_NAME = config['optimizer']['name']\n",
    "OPTIMIZER_PARAMS = config['optimizer']['params']\n",
    "\n",
    "# loss\n",
    "LOSS = config['loss']\n",
    "\n",
    "# GEL config\n",
    "LANG = config['lang']\n",
    "TOKEN_FILE = config['token_file']\n",
    "TOKEN_ERR_DISTRIBUTION = config['token_err_distribution']\n",
    "CHAR_ERR_DISTRIBUTION = config['char_err_distribution']\n",
    "TOKEN_ERR_PROB = config['token_err_prob']   \n",
    "CHAR_ERR_PROB = config['char_err_prob']\n",
    "\n",
    "# logs\n",
    "LOG_FILE = config['log_file']\n",
    "PROFILE_BATCH = config['profile_batch']\n",
    "MODEL_CHECKPOINT_PATH = config['model_checkpoint_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = introduce_errors.get_token_vocabulary(TOKEN_FILE)\n",
    "characters = introduce_errors.get_char_vocabulary(LANG)\n",
    "# aspell_speller = aspell.Speller('lang', LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new loading of dataset:\n",
    "\n",
    "# multiprocessing.set_start_method('spawn')   \n",
    "\n",
    "def split_features_and_labels(input_batch):\n",
    "    features = {key: tensor for key, tensor in input_batch.items() if key in ['input_ids', 'attention_mask', 'decoder_input_ids']}\n",
    "    labels = {key: tensor for key, tensor in input_batch.items() if key in ['labels']}\n",
    "    if len(features) == 1:\n",
    "        features = list(features.values())[0]\n",
    "    if len(labels) == 1:\n",
    "        labels = list(labels.values())[0]\n",
    "    if isinstance(labels, dict) and len(labels) == 0:\n",
    "        return features\n",
    "    else:\n",
    "        return features, labels\n",
    "\n",
    "manager = Manager()\n",
    "queue = manager.Queue(2 * NUM_PARALLEL)\n",
    "gel = load_data.GenereteErrorLine(\n",
    "        tokens, characters, LANG, \n",
    "        TOKEN_ERR_DISTRIBUTION, CHAR_ERR_DISTRIBUTION, \n",
    "        TOKEN_ERR_PROB, CHAR_ERR_PROB)\n",
    "\n",
    "process = Process(\n",
    "            target=load_data.data_generator, \n",
    "            args=(queue, DATA_PATHS, NUM_PARALLEL, gel, tokenizer, MAX_LENGTH,))\n",
    "\n",
    "process.start()\n",
    "    \n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(queue.get, None),\n",
    "    output_types={\n",
    "                \"input_ids\": tf.int32,\n",
    "                \"attention_mask\": tf.int32,\n",
    "                \"labels\": tf.int32,\n",
    "                \"decoder_input_ids\": tf.int32\n",
    "            },\n",
    "    output_shapes={\n",
    "                \"input_ids\": (None, ),\n",
    "                \"attention_mask\": (None, ),\n",
    "                \"labels\": (None, ),\n",
    "                \"decoder_input_ids\": (None, )\n",
    "            })\n",
    "\n",
    "dataset = dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
    "dataset = dataset.bucket_by_sequence_length(\n",
    "        element_length_func=lambda x, y: tf.shape(x['input_ids'])[0],\n",
    "        # bucket_boundaries=[16, 32, 48, 64, 80, 96, 112],\n",
    "        # bucket_batch_sizes=[1, 1, 1, 1 , 1 , 1 , 1, 1]\n",
    "        bucket_boundaries=[32, 64, 96],\n",
    "        bucket_batch_sizes=[76, 36, 28, 14]\n",
    ")\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # Number of batches to prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    if OPTIMIZER_NAME == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=OPTIMIZER_PARAMS['lr'])\n",
    "    elif OPTIMIZER_NAME == 'AdamW':\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=OPTIMIZER_PARAMS['lr'])\n",
    "    elif OPTIMIZER_NAME == 'Adafactor':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adafactor(learning_rate=OPTIMIZER_PARAMS['lr'])\n",
    "    elif OPTIMIZER_NAME == 'AdaptiveAdam':\n",
    "        class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, warmup_steps, d_model):\n",
    "                self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "                self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "            def __call__(self, step):\n",
    "                step = tf.cast(step, tf.float32)\n",
    "                lr = (1.0/tf.math.sqrt(self.d_model)) * tf.math.minimum(1.0 / tf.math.sqrt(step), (1.0 / tf.math.sqrt(self.warmup_steps)) * ((1.0 * step) / self.warmup_steps))\n",
    "                return lr\n",
    "\n",
    "        lr = LRSchedule(OPTIMIZER_PARAMS['warmup_steps'], MAX_LENGTH)\n",
    "        beta1 = OPTIMIZER_PARAMS['beta1']\n",
    "        beta2 = OPTIMIZER_PARAMS['beta2']\n",
    "        epsilon = OPTIMIZER_PARAMS['epsilon']\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr,\n",
    "            beta_1=beta1,\n",
    "            beta_2=beta2,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "with strategy.scope(): \n",
    "    loss = None   \n",
    "    if LOSS == \"SCC\":\n",
    "        class MaskedSparseCategoricalCrossEntropy(tf.keras.losses.Loss):\n",
    "            # source: https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/modeling_tf_utils.py#L210\n",
    "            def __init__(self, reduction=tf.keras.losses.Reduction.NONE, name=None):\n",
    "                super().__init__(reduction, name)\n",
    "                self.loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=reduction)\n",
    "            \n",
    "            def call(self, y_true, y_pred):\n",
    "                return self.hf_compute_loss(y_true, y_pred)\n",
    "\n",
    "            def hf_compute_loss(self, labels, logits):\n",
    "                unmasked_loss = self.loss_func(tf.nn.relu(labels), logits)\n",
    "                loss_mask = tf.cast(labels != -100, dtype=unmasked_loss.dtype)\n",
    "                masked_loss = unmasked_loss * loss_mask\n",
    "                reduced_masked_loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(loss_mask)\n",
    "                return reduced_masked_loss\n",
    "        \n",
    "        loss = MaskedSparseCategoricalCrossEntropy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_eval = AutoTokenizer.from_pretrained(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "    \n",
    "    if loss:\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_unchanged_words=2\n",
    "# beta = 0.5\n",
    "# ignore_whitespace_casing= False\n",
    "# verbose = False\n",
    "# very_verbose = False\n",
    "\n",
    "# dev_input = config['evaluation_input']\n",
    "# dev_gold = config['evaluation_gold']\n",
    "\n",
    "# # load source sentences and gold edits\n",
    "# fin = smart_open(dev_input, 'r')\n",
    "# dev_input_sentences = [line.strip() for line in fin.readlines()]\n",
    "# fin.close()\n",
    "\n",
    "# dev_source_sentences, dev_gold_edits = load_annotation(dev_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Evaluation(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, tokenizer, max_length, nth, max_unchanged_words, beta, ignore_whitespace_casing, verbose, very_verbose, \n",
    "#                  dev_input_sentences, dev_source_sentences, dev_gold_edits, ensure_shapes, split_features_and_labels, batch_size):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "#         self.nth = nth\n",
    "#         self.max_unchanged_words = max_unchanged_words\n",
    "#         self.beta = beta\n",
    "#         self.ignore_whitespace_casing = ignore_whitespace_casing\n",
    "#         self.verbose = verbose\n",
    "#         self.very_verbose = very_verbose\n",
    "#         self.dev_input_sentences = dev_input_sentences\n",
    "#         self.dev_source_sentences = dev_source_sentences\n",
    "#         self.dev_gold_edits = dev_gold_edits\n",
    "\n",
    "#         self.ensure_shapes = ensure_shapes\n",
    "#         self.split_features_and_labels = split_features_and_labels\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def get_tokenized_sentence(self, line):\n",
    "#         line = line.decode('utf-8')\n",
    "#         tokenized = self.tokenizer(line, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "#         return tokenized['input_ids']\n",
    "\n",
    "#     def create_tokenized_line(self, line):\n",
    "#         input_ids = tf.numpy_function(self.get_tokenized_sentence, inp=[line], Tout=tf.int32)\n",
    "#         dato = {\n",
    "#             'input_ids': input_ids[0]\n",
    "#         }\n",
    "#         return dato\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch % self.nth == 0:\n",
    "#             try:\n",
    "#                 predicted_sentences = []\n",
    "#                 val_dataset = tf.data.Dataset.from_tensor_slices(self.dev_input_sentences)\n",
    "#                 val_dataset = val_dataset.map(self.create_tokenized_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#                 val_dataset = val_dataset.map(self.ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#                 val_dataset = val_dataset.map(self.split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#                 val_dataset = val_dataset.batch(self.batch_size)\n",
    "\n",
    "#                 for batch in val_dataset: \n",
    "#                     outs = model.generate(batch)\n",
    "#                     for out in outs:\n",
    "#                         predicted_sentence = tokenizer.decode(out)\n",
    "#                         predicted_sentences.append(predicted_sentence)\n",
    "                \n",
    "#                 p, r, f1 = batch_multi_pre_rec_f1(predicted_sentences, self.dev_source_sentences, self.dev_gold_edits, \n",
    "#                                                   self.max_unchanged_words, self.beta, self.ignore_whitespace_casing, self.verbose, self.very_verbose)\n",
    "#                 print(\"Precision   : %.4f\" % p)\n",
    "#                 print(\"Recall      : %.4f\" % r)\n",
    "#                 print(\"F_%.1f       : %.4f\" % (self.beta, f1))\n",
    "#             except:\n",
    "#                 print(\"No predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# callbacks = [\n",
    "#     Evaluation(tokenizer=tokenizer_eval, max_length=MAX_LENGTH ,nth=config['evaluation_every_nth'],\n",
    "#                max_unchanged_words=max_unchanged_words, beta=beta, ignore_whitespace_casing=ignore_whitespace_casing,\n",
    "#                verbose=verbose, very_verbose=very_verbose, dev_input_sentences=dev_input_sentences, dev_source_sentences=dev_source_sentences,\n",
    "#                dev_gold_edits=dev_gold_edits, ensure_shapes=ensure_shapes, split_features_and_labels=split_features_and_labels,\n",
    "#                batch_size=config['batch_size_eval']),\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir=config['log_file'], profile_batch=config['profile_batch']),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(filepath=config['model_checkpoint_path'], save_weights_only=True, save_freq='epoch')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(MODEL_CHECKPOINT_PATH, 'ckpt-{epoch}'),\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=LOG_FILE, profile_batch=PROFILE_BATCH),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATOR:\n",
    "    sidecar = tf.keras.utils.SidecarEvaluator(\n",
    "        model,\n",
    "        dataset,\n",
    "        checkpoint_dir=MODEL_CHECKPOINT_PATH,\n",
    "        steps=300,\n",
    "        max_evaluations=None,\n",
    "        callbacks=[]\n",
    "    )\n",
    "    sidecar.start()\n",
    "\n",
    "else:\n",
    "    if STEPS_PER_EPOCH:\n",
    "        model.fit(dataset, callbacks=[model_checkpoint], epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "    else:\n",
    "        model.fit(dataset, callbacks=[model_checkpoint], epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './tmp/checkpoint/' # must be folder (/ at the end)\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     multiprocessing.set_start_method('spawn')\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
