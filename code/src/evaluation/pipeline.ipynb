{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 19:01:39.995492: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-22 19:01:40.000201: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 19:01:40.096014: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 19:01:40.096745: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 19:01:41.751681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/petr/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from create_errors import introduce_errors\n",
    "import aspell\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq \n",
    "from transformers import TFEncoderDecoderModel, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import json\n",
    "\n",
    "from m2scorer.util import paragraphs\n",
    "from m2scorer.util import smart_open\n",
    "from m2scorer.levenshtein import batch_multi_pre_rec_f1\n",
    "from m2scorer.m2scorer import load_annotation\n",
    "\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MODEL = config['model']\n",
    "DATA_PATHS = config['data_paths']\n",
    "NUM_PARALLEL = config['num_parallel']\n",
    "BATCH_SIZE_PER_REPLICE = config['batch_size_per_replica']\n",
    "MAX_LENGTH = config['max_length']\n",
    "STEPS_PER_EPOCH = config['steps_per_epoch']\n",
    "EPOCHS = config['epochs']\n",
    "SHUFFLE_BUFFER = config['shuffle_buffer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size per replica: 12\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batch size per replica: {BATCH_SIZE_PER_REPLICE}\")\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICE * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_name = config['optimizer']['name']\n",
    "optimizer_params = config['optimizer']['params']\n",
    "\n",
    "with strategy.scope():\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'Adafactor':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adafactor(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdaptiveAdam':\n",
    "        class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, warmup_steps, d_model):\n",
    "                self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "                self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "            def __call__(self, step):\n",
    "                step = tf.cast(step, tf.float32)\n",
    "                lr = (1.0/tf.math.sqrt(self.d_model)) * tf.math.minimum(1.0 / tf.math.sqrt(step), (1.0 / tf.math.sqrt(self.warmup_steps)) * ((1.0 * step) / self.warmup_steps))\n",
    "                return lr\n",
    "\n",
    "        lr = LRSchedule(optimizer_params['warmup_steps'], MAX_LENGTH)\n",
    "        beta1 = optimizer_params['beta1']\n",
    "        beta2 = optimizer_params['beta2']\n",
    "        epsilon = optimizer_params['epsilon']\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr,\n",
    "            beta_1=beta1,\n",
    "            beta_2=beta2,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "with strategy.scope(): \n",
    "    loss = None   \n",
    "    if config['loss'] == \"SCC\":\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = config['lang']\n",
    "token_file = config['token_file']\n",
    "tokens = introduce_errors.get_token_vocabulary(token_file)\n",
    "characters = introduce_errors.get_char_vocabulary(lang)\n",
    "aspell_speller = aspell.Speller('lang', lang)\n",
    "token_err_distribution = config['token_err_distribution']\n",
    "char_err_distribution = config['char_err_distribution']\n",
    "token_err_prob = config['token_err_prob']   \n",
    "char_err_prob = config['char_err_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petr/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenereteErrorLine():\n",
    "\n",
    "    def __init__(self, tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob, token_std_dev=0.2, char_std_dev=0.01):\n",
    "        self.tokens = tokens\n",
    "        self.characters = characters\n",
    "        self.aspell_speller = aspell_speller\n",
    "        self.token_err_distribution = token_err_distribution\n",
    "        self.char_err_distribution = char_err_distribution\n",
    "        self.token_err_prob = token_err_prob\n",
    "        self.token_std_dev = token_std_dev\n",
    "        self.char_err_prob = char_err_prob\n",
    "        self.char_std_dev = char_std_dev\n",
    "\n",
    "    def __call__(self, line):\n",
    "        line = line.decode('utf-8')\n",
    "        token_replace_prob, token_insert_prob, token_delete_prob, token_swap_prob, recase_prob = self.token_err_distribution\n",
    "        char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob, change_diacritics_prob = self.char_err_distribution\n",
    "        line = line.strip('\\n')\n",
    "        \n",
    "        # introduce word-level errors\n",
    "        line = introduce_errors.introduce_token_level_errors_on_sentence(line.split(' '), token_replace_prob, token_insert_prob, token_delete_prob,\n",
    "                                                        token_swap_prob, recase_prob, float(self.token_err_prob), float(self.token_std_dev),\n",
    "                                                        self.tokens, self.aspell_speller)\n",
    "        if '\\t' in line or '\\n' in line:\n",
    "            raise ValueError('!!! Error !!! ' + line)\n",
    "        # introduce spelling errors\n",
    "        line = introduce_errors.introduce_char_level_errors_on_sentence(line, char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob,\n",
    "                                                       change_diacritics_prob, float(self.char_err_prob), float(self.char_std_dev),\n",
    "                                                       self.characters)\n",
    "        return line\n",
    "    \n",
    "gel = GenereteErrorLine(tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(error_line, label_line):\n",
    "    error_line = error_line.decode('utf-8')\n",
    "    label_line = label_line.decode('utf-8')\n",
    "    tokenized = tokenizer(error_line, text_target=label_line, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    return tokenized['input_ids'], tokenized['attention_mask'], tokenized['labels']\n",
    "\n",
    "def create_error_line(line):\n",
    "    error_line = tf.numpy_function(gel, inp=[line], Tout=[tf.string])[0]\n",
    "    label_line = line\n",
    "    input_ids, attention_mask, labels = tf.numpy_function(get_tokenized_sentences, inp=[error_line, label_line], Tout=[tf.int32, tf.int32, tf.int32])\n",
    "    decoder_input_ids = tf.roll(labels, shift=1, axis=1)\n",
    "    dato = {\n",
    "        'input_ids': input_ids[0],\n",
    "        'attention_mask': attention_mask[0],\n",
    "        'decoder_input_ids': decoder_input_ids[0],\n",
    "        'labels': labels[0]\n",
    "    }\n",
    "    return dato\n",
    "\n",
    "def ensure_shapes(input_dict):\n",
    "    return {key: tf.ensure_shape(val, (MAX_LENGTH)) for key, val in input_dict.items()}\n",
    "\n",
    "def split_features_and_labels(input_batch):\n",
    "    features = {key: tensor for key, tensor in input_batch.items() if key in ['input_ids', 'attention_mask', 'decoder_input_ids']}\n",
    "    labels = {key: tensor for key, tensor in input_batch.items() if key in ['labels']}\n",
    "    if len(features) == 1:\n",
    "        features = list(features.values())[0]\n",
    "    if len(labels) == 1:\n",
    "        labels = list(labels.values())[0]\n",
    "    if isinstance(labels, dict) and len(labels) == 0:\n",
    "        return features\n",
    "    else:\n",
    "        return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(DATA_PATHS, num_parallel_reads=NUM_PARALLEL)\n",
    "\n",
    "dataset = dataset.map(create_error_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.ignore_errors()\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petr/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    if config[\"pretrained\"]:\n",
    "        model = TFAutoModelForSeq2SeqLM.from_pretrained(config['model'])\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(config['model'])\n",
    "        model = TFAutoModelForSeq2SeqLM.from_config(config)\n",
    "    \n",
    "    if loss:\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_unchanged_words=2\n",
    "beta = 0.5\n",
    "ignore_whitespace_casing= False\n",
    "verbose = False\n",
    "very_verbose = False\n",
    "\n",
    "dev_input = config['evaluation_input']\n",
    "dev_gold = config['evaluation_gold']\n",
    "\n",
    "# load source sentences and gold edits\n",
    "fin = smart_open(dev_input, 'r')\n",
    "dev_input_sentences = [line.strip() for line in fin.readlines()]\n",
    "fin.close()\n",
    "\n",
    "dev_source_sentences, dev_gold_edits = load_annotation(dev_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 19:34:59.046683: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-04-22 19:34:59.046796: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-04-22 19:34:59.048208: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "class Evaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, nth, max_unchanged_words, beta, ignore_whitespace_casing, verbose, very_verbose, \n",
    "                 dev_input_sentences, dev_source_sentences, dev_gold_edits, ensure_shapes, split_features_and_labels, batch_size):\n",
    "        self.tokenzer = tokenizer\n",
    "        self.nth = nth\n",
    "        self.max_unchanged_words = max_unchanged_words\n",
    "        self.beta = beta\n",
    "        self.ignore_whitespace_casing = ignore_whitespace_casing\n",
    "        self.verbose = verbose\n",
    "        self.very_verbose = very_verbose\n",
    "        self.dev_input_sentences = dev_input_sentences\n",
    "        self.dev_source_sentences = dev_source_sentences\n",
    "        self.dev_gold_edits = dev_gold_edits\n",
    "\n",
    "        self.ensure_shapes = ensure_shapes\n",
    "        self.split_features_and_labels = split_features_and_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_tokenized_sentence(self, line):\n",
    "        line = line.decode('utf-8')\n",
    "        tokenized = tokenizer(line, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "        return tokenized['input_ids']\n",
    "\n",
    "    def create_tokenized_line(self, line):\n",
    "        input_ids = tf.numpy_function(self.get_tokenized_sentence, inp=[line], Tout=tf.int32)\n",
    "        dato = {\n",
    "            'input_ids': input_ids[0]\n",
    "        }\n",
    "        return dato\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.nth == 0:\n",
    "            try:\n",
    "                predicted_sentences = []\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices(self.dev_input_sentences)\n",
    "                val_dataset = val_dataset.map(self.create_tokenized_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                val_dataset = val_dataset.map(self.ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                val_dataset = val_dataset.map(self.split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                val_dataset = val_dataset.batch(self.batch_size)\n",
    "\n",
    "                for batch in val_dataset: \n",
    "                    outs = model.generate(batch)\n",
    "                    for out in outs:\n",
    "                        predicted_sentence = tokenizer.decode(out)\n",
    "                        predicted_sentences.append(predicted_sentence)\n",
    "                \n",
    "                p, r, f1 = batch_multi_pre_rec_f1(predicted_sentences, self.dev_source_sentences, self.dev_gold_edits, \n",
    "                                                  self.max_unchanged_words, self.beta, self.ignore_whitespace_casing, self.verbose, self.very_verbose)\n",
    "                print(\"Precision   : %.4f\" % p)\n",
    "                print(\"Recall      : %.4f\" % r)\n",
    "                print(\"F_%.1f       : %.4f\" % (self.beta, f1))\n",
    "            except:\n",
    "                print(\"No predictions...\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluation(tokenizer=tokenizer, nth=config['evaluation_every_nth'],\n",
    "               max_unchanged_words=max_unchanged_words, beta=beta, ignore_whitespace_casing=ignore_whitespace_casing,\n",
    "               verbose=verbose, very_verbose=very_verbose, dev_input_sentences=dev_input_sentences, dev_source_sentences=dev_source_sentences,\n",
    "               dev_gold_edits=dev_gold_edits, ensure_shapes=ensure_shapes, split_features_and_labels=split_features_and_labels,\n",
    "               batch_size=config['batch_size_eval']),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=config['log_file'], profile_batch=config['profile_batch']),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=config['model_checkpoint_path'], save_weights_only=True, save_freq='epoch')\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence(line):\n",
    "    line = line.decode('utf-8')\n",
    "    tokenized = tokenizer(line, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    return tokenized['input_ids']\n",
    "\n",
    "def create_tokenized_line(line):\n",
    "    input_ids = tf.numpy_function(get_tokenized_sentence, inp=[line], Tout=tf.int32)\n",
    "    dato = {\n",
    "        'input_ids': input_ids[0]\n",
    "    }\n",
    "    return dato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices([\n",
    "    \"Mám rád\", \n",
    "    \"Mám rád modruo barvu\"\n",
    "    ])\n",
    "\n",
    " # vetsi batch ~ rychlejsi evaluace\n",
    "val_dataset = val_dataset.map(create_tokenized_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 19:39:01.529889: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-22 19:39:01.530683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "/home/petr/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/transformers/generation/tf_utils.py:854: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = []\n",
    "for a in val_dataset:\n",
    "    pr.append(model.generate(a))\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 20), dtype=int32)\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 20), dtype=int32)\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for p in pr:\n",
    "    print(p)\n",
    "    d = tokenizer.decode(p[0])\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m neco \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39mstack(pr, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, pr[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m----> 2\u001b[0m tokenizer\u001b[39m.\u001b[39;49mdecode(neco)\n",
      "File \u001b[0;32m~/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3486\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3483\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3484\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3486\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3487\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3488\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3489\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3490\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3491\u001b[0m )\n",
      "File \u001b[0;32m~/Plocha/DP/czech-gec/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:549\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    548\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 549\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    551\u001b[0m clean_up_tokenization_spaces \u001b[39m=\u001b[39m (\n\u001b[1;32m    552\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    553\u001b[0m     \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "\n",
    "neco = tf.reshape(tf.stack(pr, axis=0), (-1, pr[0].shape[1]))\n",
    "tokenizer.decode(neco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Mám rád mého pas\", \n",
    "    \"Mám rád modruo barvu\"\n",
    "    ]\n",
    "\n",
    "tokenized = tokenizer(sentences, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[ 6809,   282, 48482,   326,  4948,   886,     1,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [ 6809,   282, 48482, 61589,   273,   268, 33173,   273,     1,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.constant(preds.logits[0])\n",
    "out = tf.keras.activations.softmax(input, axis=1)\n",
    "print(out) # dodelat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEPS_PER_EPOCH:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "else:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './tmp/checkpoint/' # must be folder (/ at the end)\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
