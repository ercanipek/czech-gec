{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from create_errors import introduce_errors\n",
    "import aspell\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq \n",
    "from transformers import TFEncoderDecoderModel, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import json\n",
    "\n",
    "from m2scorer.util import paragraphs\n",
    "from m2scorer.util import smart_open\n",
    "from m2scorer.levenshtein import batch_multi_pre_rec_f1\n",
    "from m2scorer.m2scorer import load_annotation\n",
    "\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MODEL = config['model']\n",
    "DATA_PATHS = config['data_paths']\n",
    "NUM_PARALLEL = config['num_parallel']\n",
    "BATCH_SIZE_PER_REPLICE = config['batch_size_per_replica']\n",
    "MAX_LENGTH = config['max_length']\n",
    "STEPS_PER_EPOCH = config['steps_per_epoch']\n",
    "EPOCHS = config['epochs']\n",
    "SHUFFLE_BUFFER = config['shuffle_buffer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch size per replica: {BATCH_SIZE_PER_REPLICE}\")\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICE * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_name = config['optimizer']['name']\n",
    "optimizer_params = config['optimizer']['params']\n",
    "\n",
    "with strategy.scope():\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'Adafactor':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adafactor(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdaptiveAdam':\n",
    "        class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, warmup_steps, d_model):\n",
    "                self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "                self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "            def __call__(self, step):\n",
    "                step = tf.cast(step, tf.float32)\n",
    "                lr = (1.0/tf.math.sqrt(self.d_model)) * tf.math.minimum(1.0 / tf.math.sqrt(step), (1.0 / tf.math.sqrt(self.warmup_steps)) * ((1.0 * step) / self.warmup_steps))\n",
    "                return lr\n",
    "\n",
    "        lr = LRSchedule(optimizer_params['warmup_steps'], MAX_LENGTH)\n",
    "        beta1 = optimizer_params['beta1']\n",
    "        beta2 = optimizer_params['beta2']\n",
    "        epsilon = optimizer_params['epsilon']\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr,\n",
    "            beta_1=beta1,\n",
    "            beta_2=beta2,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "with strategy.scope(): \n",
    "    loss = None   \n",
    "    if config['loss'] == \"SCC\":\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = config['lang']\n",
    "token_file = config['token_file']\n",
    "tokens = introduce_errors.get_token_vocabulary(token_file)\n",
    "characters = introduce_errors.get_char_vocabulary(lang)\n",
    "aspell_speller = aspell.Speller('lang', lang)\n",
    "token_err_distribution = config['token_err_distribution']\n",
    "char_err_distribution = config['char_err_distribution']\n",
    "token_err_prob = config['token_err_prob']   \n",
    "char_err_prob = config['char_err_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenereteErrorLine():\n",
    "\n",
    "    def __init__(self, tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob, token_std_dev=0.2, char_std_dev=0.01):\n",
    "        self.tokens = tokens\n",
    "        self.characters = characters\n",
    "        self.aspell_speller = aspell_speller\n",
    "        self.token_err_distribution = token_err_distribution\n",
    "        self.char_err_distribution = char_err_distribution\n",
    "        self.token_err_prob = token_err_prob\n",
    "        self.token_std_dev = token_std_dev\n",
    "        self.char_err_prob = char_err_prob\n",
    "        self.char_std_dev = char_std_dev\n",
    "\n",
    "    def __call__(self, line):\n",
    "        line = line.decode('utf-8')\n",
    "        token_replace_prob, token_insert_prob, token_delete_prob, token_swap_prob, recase_prob = self.token_err_distribution\n",
    "        char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob, change_diacritics_prob = self.char_err_distribution\n",
    "        line = line.strip('\\n')\n",
    "        \n",
    "        # introduce word-level errors\n",
    "        line = introduce_errors.introduce_token_level_errors_on_sentence(line.split(' '), token_replace_prob, token_insert_prob, token_delete_prob,\n",
    "                                                        token_swap_prob, recase_prob, float(self.token_err_prob), float(self.token_std_dev),\n",
    "                                                        self.tokens, self.aspell_speller)\n",
    "        if '\\t' in line or '\\n' in line:\n",
    "            raise ValueError('!!! Error !!! ' + line)\n",
    "        # introduce spelling errors\n",
    "        line = introduce_errors.introduce_char_level_errors_on_sentence(line, char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob,\n",
    "                                                       change_diacritics_prob, float(self.char_err_prob), float(self.char_std_dev),\n",
    "                                                       self.characters)\n",
    "        return line\n",
    "    \n",
    "gel = GenereteErrorLine(tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(error_line, label_line):\n",
    "    error_line = error_line.decode('utf-8')\n",
    "    label_line = label_line.decode('utf-8')\n",
    "    tokenized = tokenizer(error_line, text_target=label_line, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    return tokenized['input_ids'], tokenized['attention_mask'], tokenized['labels']\n",
    "\n",
    "def create_error_line(line):\n",
    "    error_line = tf.numpy_function(gel, inp=[line], Tout=[tf.string])[0]\n",
    "    label_line = line\n",
    "    input_ids, attention_mask, labels = tf.numpy_function(get_tokenized_sentences, inp=[error_line, label_line], Tout=[tf.int32, tf.int32, tf.int32])\n",
    "    decoder_input_ids = tf.roll(labels, shift=1, axis=1)\n",
    "    dato = {\n",
    "        'input_ids': input_ids[0],\n",
    "        'attention_mask': attention_mask[0],\n",
    "        'decoder_input_ids': decoder_input_ids[0],\n",
    "        'labels': labels[0]\n",
    "    }\n",
    "    return dato\n",
    "\n",
    "def ensure_shapes(input_dict):\n",
    "    return {key: tf.ensure_shape(val, (MAX_LENGTH)) for key, val in input_dict.items()}\n",
    "\n",
    "def split_features_and_labels(input_batch):\n",
    "    features = {key: tensor for key, tensor in input_batch.items() if key in ['input_ids', 'attention_mask', 'decoder_input_ids']}\n",
    "    labels = {key: tensor for key, tensor in input_batch.items() if key in ['labels']}\n",
    "    if len(features) == 1:\n",
    "        features = list(features.values())[0]\n",
    "    if len(labels) == 1:\n",
    "        labels = list(labels.values())[0]\n",
    "    if isinstance(labels, dict) and len(labels) == 0:\n",
    "        return features\n",
    "    else:\n",
    "        return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(DATA_PATHS, num_parallel_reads=NUM_PARALLEL)\n",
    "\n",
    "dataset = dataset.map(create_error_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.ignore_errors()\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    if config[\"pretrained\"]:\n",
    "        model = TFAutoModelForSeq2SeqLM.from_pretrained(config['model'])\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(config['model'])\n",
    "        model = TFAutoModelForSeq2SeqLM.from_config(config)\n",
    "    \n",
    "    if loss:\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_unchanged_words=2\n",
    "beta = 0.5\n",
    "ignore_whitespace_casing= False\n",
    "verbose = False\n",
    "very_verbose = False\n",
    "\n",
    "dev_input = config['evaluation_input']\n",
    "dev_gold = config['evaluation_gold']\n",
    "\n",
    "# load source sentences and gold edits\n",
    "fin = smart_open(dev_input, 'r')\n",
    "dev_input_sentences = [line.strip() for line in fin.readlines()]\n",
    "fin.close()\n",
    "\n",
    "dev_source_sentences, dev_gold_edits = load_annotation(dev_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, nth, max_unchanged_words, beta, ignore_whitespace_casing, verbose, very_verbose, \n",
    "                 dev_input_sentences, dev_source_sentences, dev_gold_edits):\n",
    "        self.tokenzer = tokenizer\n",
    "        self.nth = nth\n",
    "        self.max_unchanged_words = max_unchanged_words\n",
    "        self.beta = beta\n",
    "        self.ignore_whitespace_casing = ignore_whitespace_casing\n",
    "        self.verbose = verbose\n",
    "        self.very_verbose = very_verbose\n",
    "        self.dev_input_sentences = dev_input_sentences\n",
    "        self.dev_source_sentences = dev_source_sentences\n",
    "        self.dev_gold_edits = dev_gold_edits\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.nth == 0:\n",
    "            try:\n",
    "                predicted_sentences = []\n",
    "                for sentence in self.dev_input_sentences: \n",
    "                    tokenized_sentence = tokenizer(sentence, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "                    output = model.generate(tokenized_sentence['input_ids'])\n",
    "                    predicted_sentence =  tokenizer.decode(output[0])\n",
    "                    predicted_sentences.append(predicted_sentence)\n",
    "                \n",
    "                p, r, f1 = batch_multi_pre_rec_f1(predicted_sentences, self.dev_source_sentences, self.dev_gold_edits, \n",
    "                                                  self.max_unchanged_words, self.beta, self.ignore_whitespace_casing, self.verbose, self.very_verbose)\n",
    "                print(\"Precision   : %.4f\" % p)\n",
    "                print(\"Recall      : %.4f\" % r)\n",
    "                print(\"F_%.1f       : %.4f\" % (self.beta, f1))\n",
    "            except:\n",
    "                print(\"No predictions...\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluation(tokenizer=tokenizer, nth=config['evaluation_every_nth'],\n",
    "               max_unchanged_words=max_unchanged_words, beta=beta, ignore_whitespace_casing=ignore_whitespace_casing,\n",
    "               verbose=verbose, very_verbose=very_verbose, dev_input_sentences=dev_input_sentences, dev_source_sentences=dev_source_sentences,\n",
    "               dev_gold_edits=dev_gold_edits),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=config['log_file'], profile_batch=config['profile_batch']),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=config['model_checkpoint_path'], save_weights_only=True, save_freq='epoch')\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence(line):\n",
    "    line = line.decode('utf-8')\n",
    "    tokenized = tokenizer(line, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    return tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "def create_tokenized_line(line):\n",
    "    input_ids, attention_mask = tf.numpy_function(get_tokenized_sentence, inp=[line], Tout=[tf.int32, tf.int32])\n",
    "    decoder_input_ids = tf.roll(input_ids, shift=1, axis=1)\n",
    "    dato = {\n",
    "        'input_ids': input_ids[0],\n",
    "        'attention_mask': attention_mask[0],\n",
    "        'decoder_input_ids': decoder_input_ids[0],\n",
    "    }\n",
    "    return dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices([\n",
    "    \"Mám rád mého pas\", \n",
    "    \"Mám rád modruo barvu\"\n",
    "    ])\n",
    "\n",
    "val_dataset = val_dataset.map(create_tokenized_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.constant(preds.logits[0])\n",
    "out = tf.keras.activations.softmax(input, axis=1)\n",
    "tf.re\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"Mám rád mého pas\"\n",
    "tokenized_sentence = tokenizer(sentence, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "tokenized_sentence['decoder_input_ids'] = model._shift_right(tokenized_sentence['input_ids'])\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEPS_PER_EPOCH:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "else:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './tmp/checkpoint/' # must be folder (/ at the end)\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
