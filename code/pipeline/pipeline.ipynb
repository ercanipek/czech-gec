{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from create_errors import introduce_errors\n",
    "import aspell\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq \n",
    "from transformers import TFEncoderDecoderModel, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MODEL = config['model']\n",
    "DATA_PATHS = config['data_paths']\n",
    "NUM_PARALLEL = config['num_parallel']\n",
    "BATCH_SIZE_PER_REPLICE = config['batch_size_per_replica']\n",
    "MAX_LENGTH = config['max_length']\n",
    "STEPS_PER_EPOCH = config['steps_per_epoch']\n",
    "EPOCHS = config['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch size per replica: {BATCH_SIZE_PER_REPLICE}\")\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICE * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_name = config['optimizer']['name']\n",
    "optimizer_params = config['optimizer']['params']\n",
    "\n",
    "with strategy.scope():\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'Adafactor':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adafactor(learning_rate=optimizer_params['lr'])\n",
    "    elif optimizer_name == 'AdaptiveAdam':\n",
    "        class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, warmup_steps, d_model):\n",
    "                self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "                self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "            def __call__(self, step):\n",
    "                step = tf.cast(step, tf.float32)\n",
    "                lr = (1.0/tf.math.sqrt(self.d_model)) * tf.math.minimum(1.0 / tf.math.sqrt(step), (1.0 / tf.math.sqrt(self.warmup_steps)) * ((1.0 * step) / self.warmup_steps))\n",
    "                return lr\n",
    "\n",
    "        lr = LRSchedule(optimizer_params['warmup_steps'], MAX_LENGTH)\n",
    "        beta1 = optimizer_params['beta1']\n",
    "        beta2 = optimizer_params['beta2']\n",
    "        epsilon = optimizer_params['epsilon']\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr,\n",
    "            beta_1=beta1,\n",
    "            beta_2=beta2,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "with strategy.scope(): \n",
    "    loss = None   \n",
    "    if config['loss'] == \"SCC\":\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = config['lang']\n",
    "token_file = config['token_file']\n",
    "tokens = introduce_errors.get_token_vocabulary(token_file)\n",
    "characters = introduce_errors.get_char_vocabulary(lang)\n",
    "aspell_speller = aspell.Speller('lang', lang)\n",
    "token_err_distribution = config['token_err_distribution']\n",
    "char_err_distribution = config['char_err_distribution']\n",
    "token_err_prob = config['token_err_prob']   \n",
    "char_err_prob = config['char_err_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenereteErrorLine():\n",
    "\n",
    "    def __init__(self, tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob, token_std_dev=0.2, char_std_dev=0.01):\n",
    "        self.tokens = tokens\n",
    "        self.characters = characters\n",
    "        self.aspell_speller = aspell_speller\n",
    "        self.token_err_distribution = token_err_distribution\n",
    "        self.char_err_distribution = char_err_distribution\n",
    "        self.token_err_prob = token_err_prob\n",
    "        self.token_std_dev = token_std_dev\n",
    "        self.char_err_prob = char_err_prob\n",
    "        self.char_std_dev = char_std_dev\n",
    "\n",
    "    def __call__(self, line):\n",
    "        line = line.decode('utf-8')\n",
    "        token_replace_prob, token_insert_prob, token_delete_prob, token_swap_prob, recase_prob = self.token_err_distribution\n",
    "        char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob, change_diacritics_prob = self.char_err_distribution\n",
    "        line = line.strip('\\n')\n",
    "        \n",
    "        # introduce word-level errors\n",
    "        line = introduce_errors.introduce_token_level_errors_on_sentence(line.split(' '), token_replace_prob, token_insert_prob, token_delete_prob,\n",
    "                                                        token_swap_prob, recase_prob, float(self.token_err_prob), float(self.token_std_dev),\n",
    "                                                        self.tokens, self.aspell_speller)\n",
    "        if '\\t' in line or '\\n' in line:\n",
    "            raise ValueError('!!! Error !!! ' + line)\n",
    "        # introduce spelling errors\n",
    "        line = introduce_errors.introduce_char_level_errors_on_sentence(line, char_replace_prob, char_insert_prob, char_delete_prob, char_swap_prob,\n",
    "                                                       change_diacritics_prob, float(self.char_err_prob), float(self.char_std_dev),\n",
    "                                                       self.characters)\n",
    "        return line\n",
    "    \n",
    "gel = GenereteErrorLine(tokens, characters, aspell_speller, token_err_distribution, char_err_distribution, token_err_prob, char_err_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(error_line, label_line):\n",
    "    error_line = error_line.decode('utf-8')\n",
    "    label_line = label_line.decode('utf-8')\n",
    "    tokenized = tokenizer(error_line, text_target=label_line, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "    return tokenized['input_ids'], tokenized['attention_mask'], tokenized['labels']\n",
    "\n",
    "def create_error_line(line):\n",
    "    error_line = tf.numpy_function(gel, inp=[line], Tout=[tf.string])[0]\n",
    "    label_line = line\n",
    "    input_ids, attention_mask, labels = tf.numpy_function(get_tokenized_sentences, inp=[error_line, label_line], Tout=[tf.int32, tf.int32, tf.int32])\n",
    "    decoder_input_ids = tf.roll(labels, shift=1, axis=1)\n",
    "    dato = {\n",
    "        'input_ids': input_ids[0],\n",
    "        'attention_mask': attention_mask[0],\n",
    "        'decoder_input_ids': decoder_input_ids[0],\n",
    "        'labels': labels[0]\n",
    "    }\n",
    "    return dato\n",
    "\n",
    "def ensure_shapes(input_dict):\n",
    "    return {key: tf.ensure_shape(val, (MAX_LENGTH)) for key, val in input_dict.items()}\n",
    "\n",
    "def split_features_and_labels(input_batch):\n",
    "    features = {key: tensor for key, tensor in input_batch.items() if key in ['input_ids', 'attention_mask', 'decoder_input_ids']}\n",
    "    labels = {key: tensor for key, tensor in input_batch.items() if key in ['labels']}\n",
    "    if len(features) == 1:\n",
    "        features = list(features.values())[0]\n",
    "    if len(labels) == 1:\n",
    "        labels = list(labels.values())[0]\n",
    "    if isinstance(labels, dict) and len(labels) == 0:\n",
    "        return features\n",
    "    else:\n",
    "        return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(DATA_PATHS, num_parallel_reads=NUM_PARALLEL)\n",
    "\n",
    "dataset = dataset.map(create_error_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(ensure_shapes, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(split_features_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.ignore_errors()\n",
    "dataset = dataset.shuffle(500)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    if config[\"pretrained\"]:\n",
    "        model = TFAutoModelForSeq2SeqLM.from_pretrained(config['model'])\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(config['model'])\n",
    "        model = TFAutoModelForSeq2SeqLM.from_config(config)\n",
    "    \n",
    "    if loss:\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, nth):\n",
    "        self.tokenzer = tokenizer\n",
    "        self.nth = nth\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.nth == 0:\n",
    "            try:\n",
    "                sentences = [\n",
    "                    \"Mam rad svoju mamkinku .\",\n",
    "                    \"Moj pseik je moc roztomilý .\",\n",
    "                    \"Nejim zelneou zeleninu .\",\n",
    "                    \"Temé čelo mu se pokrylo potem .\",\n",
    "                    \"Vypadalo to , že každou omdlí .\",\n",
    "                    \"Bože , dEe , ty vypadáš bječně ! \"\n",
    "                ]\n",
    "                print()\n",
    "                for sentence in sentences: \n",
    "                    tokenized_sentence = tokenizer(sentence, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "                    output = model.generate(tokenized_sentence['input_ids'])\n",
    "                    print(tokenizer.decode(output[0]))\n",
    "                    print()\n",
    "            except:\n",
    "                print(\"No predictions...\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluation(tokenizer=tokenizer, nth=config['evaluation_every_nth']),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=config['log_file'], profile_batch=config['profile_batch']),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=config['model_checkpoint_path'], save_weights_only=True, save_freq='epoch')\n",
    "]\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(config['log_file'], 'ckpt-{epoch}'),\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEPS_PER_EPOCH:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "else:\n",
    "    model.fit(dataset, callbacks=callbacks, epochs=EPOCHS)\n",
    "\n",
    "# if STEPS_PER_EPOCH:\n",
    "#     model.fit(dataset, callbacks=[model_checkpoint], epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "# else:\n",
    "#     model.fit(dataset, callbacks=[model_checkpoint], epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.SidecarEvaluator(\n",
    "#     model=model,\n",
    "#     data=dataset,\n",
    "#     checkpoint_dir=config['log_file'],\n",
    "#     steps=None,\n",
    "#     max_evaluations=None,\n",
    "#     callbacks=callbacks\n",
    "# ).start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './tmp/checkpoint/' # must be folder (/ at the end)\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
